{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b1b3330",
      "metadata": {},
      "source": [
        "# K-Means Upvote Clustering Lab\n",
        "\n",
        "Explore Hacker News post engagement clusters using the processed dataset. This notebook mirrors the CLI script (notebooks/kmeans_lab.py) by loading data, fitting K-Means, labelling clusters, and evaluating how well the clusters surface high-upvote behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a216e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12d93dc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment settings\n",
        "DATASET_PATH = Path('data/processed/combined_dataset.csv')\n",
        "REPORTS_DIR = Path('reports')\n",
        "\n",
        "N_CLUSTERS = 4\n",
        "TEST_SIZE = 0.1\n",
        "RANDOM_STATE = 42\n",
        "OUTPUT_CSV = REPORTS_DIR / 'kmeans_holdout_comparison.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa37dad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ClusterSummary:\n",
        "    cluster: int\n",
        "    mean_points: float\n",
        "    median_points: float\n",
        "    count: int\n",
        "    category: str\n",
        "\n",
        "\n",
        "def load_dataset(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(subset=['title', 'comments_count', 'points', 'method'])\n",
        "    df['title_length'] = df['title'].str.len()\n",
        "    df['top_comment_length'] = df['top_comment_text'].fillna('').str.len()\n",
        "    df['author_encoded'] = df['author'].fillna('unknown')\n",
        "    df['has_comment'] = (df['top_comment_text'].fillna('').str.len() > 0).astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_pipeline(n_clusters: int, random_state: int) -> Pipeline:\n",
        "    numeric_features = [\n",
        "        'comments_count',\n",
        "        'title_length',\n",
        "        'top_comment_length',\n",
        "        'has_comment',\n",
        "    ]\n",
        "    categorical_features = ['method', 'author_encoded']\n",
        "\n",
        "    preprocess = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_features),\n",
        "            (\n",
        "                'cat',\n",
        "                OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
        "                categorical_features,\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')\n",
        "    return Pipeline(\n",
        "        steps=[\n",
        "            ('preprocess', preprocess),\n",
        "            ('cluster', model),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def label_clusters(train_df: pd.DataFrame, labels: np.ndarray) -> List[ClusterSummary]:\n",
        "    grouped = (\n",
        "        train_df.assign(cluster=labels)\n",
        "        .groupby('cluster')['points']\n",
        "        .agg(['mean', 'median', 'count'])\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    means = grouped['mean']\n",
        "    high_cut = means.quantile(0.66)\n",
        "    low_cut = means.quantile(0.33)\n",
        "\n",
        "    def categorize(mean_val: float) -> str:\n",
        "        if mean_val >= high_cut:\n",
        "            return 'High-Upvote Cluster'\n",
        "        if mean_val <= low_cut:\n",
        "            return 'Low-Upvote Cluster'\n",
        "        return 'Mid-Upvote Cluster'\n",
        "\n",
        "    return [\n",
        "        ClusterSummary(\n",
        "            cluster=int(row['cluster']),\n",
        "            mean_points=float(row['mean']),\n",
        "            median_points=float(row['median']),\n",
        "            count=int(row['count']),\n",
        "            category=categorize(row['mean']),\n",
        "        )\n",
        "        for _, row in grouped.iterrows()\n",
        "    ]\n",
        "\n",
        "\n",
        "def summarize_holdout(\n",
        "    holdout_df: pd.DataFrame,\n",
        "    holdout_labels: np.ndarray,\n",
        "    summaries: List[ClusterSummary],\n",
        ") -> Tuple[pd.DataFrame, float]:\n",
        "    summary_map = {s.cluster: s for s in summaries}\n",
        "\n",
        "    holdout = holdout_df.copy()\n",
        "    holdout['cluster'] = holdout_labels\n",
        "    holdout['cluster_mean_points'] = holdout['cluster'].map(\n",
        "        lambda c: summary_map[c].mean_points\n",
        "    )\n",
        "    holdout['cluster_category'] = holdout['cluster'].map(\n",
        "        lambda c: summary_map[c].category\n",
        "    )\n",
        "\n",
        "    global_median = holdout_df['points'].median()\n",
        "    actual_high = holdout_df['points'] >= global_median\n",
        "    predicted_high = holdout['cluster_category'] == 'High-Upvote Cluster'\n",
        "    accuracy = accuracy_score(actual_high.astype(int), predicted_high.astype(int))\n",
        "\n",
        "    return holdout, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d140b99",
      "metadata": {},
      "outputs": [],
      "source": [
        "features = [\n",
        "    'comments_count',\n",
        "    'title_length',\n",
        "    'top_comment_length',\n",
        "    'has_comment',\n",
        "    'method',\n",
        "    'author_encoded',\n",
        "]\n",
        "\n",
        "df = load_dataset(DATASET_PATH)\n",
        "train_df, holdout_df = train_test_split(\n",
        "    df,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=df['method'],\n",
        ")\n",
        "\n",
        "pipeline = build_pipeline(N_CLUSTERS, RANDOM_STATE)\n",
        "pipeline.fit(train_df[features])\n",
        "\n",
        "cluster_labels = pipeline.named_steps['cluster'].labels_\n",
        "summaries = label_clusters(train_df.reset_index(drop=True), cluster_labels)\n",
        "\n",
        "holdout_labels = pipeline.predict(holdout_df[features])\n",
        "holdout_results, accuracy = summarize_holdout(\n",
        "    holdout_df.reset_index(drop=True),\n",
        "    holdout_labels,\n",
        "    summaries,\n",
        ")\n",
        "\n",
        "summary_df = (\n",
        "    pd.DataFrame([s.__dict__ for s in summaries])\n",
        "    .sort_values('mean_points', ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "display(summary_df)\n",
        "display(\n",
        "    holdout_results[\n",
        "        [\n",
        "            'post_id',\n",
        "            'title',\n",
        "            'points',\n",
        "            'comments_count',\n",
        "            'method',\n",
        "            'cluster',\n",
        "            'cluster_category',\n",
        "            'cluster_mean_points',\n",
        "        ]\n",
        "    ].head()\n",
        ")\n",
        "print(f'Holdout high-upvote agreement accuracy: {accuracy:.2%}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f30de8",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "holdout_results[\n",
        "    [\n",
        "        'post_id',\n",
        "        'title',\n",
        "        'points',\n",
        "        'comments_count',\n",
        "        'method',\n",
        "        'cluster',\n",
        "        'cluster_category',\n",
        "        'cluster_mean_points',\n",
        "    ]\n",
        "].to_csv(OUTPUT_CSV, index=False)\n",
        "print(f'Detailed results saved to {OUTPUT_CSV}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
